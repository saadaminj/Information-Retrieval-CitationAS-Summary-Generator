-Citation contents are the research papers which a researches read before writing a new research paper, citation content is present in the end of the newly written research paper.
-Citation sentences are sentences which refers to a previous research paper. 

reference is given as:
in research paper "newRP" -> content : "infections of dog [1]"
in the end of the research paper "newRP":
[1] - Talan, David A., et al. "Bacteriologic analysis of infected dog." New England Journal of Medicine 340.2 (1999): 85-92.

-citation sentence : "infections of dog" 
-citation content : the whole research paper by Talan and David about "Bateriologic analysis of infected dog"

Electronic academic literatures are present in abundant quantity. when researches start to investigate into new topics they had to read previous researches to analyse how much the field has progressed over time, and where it stands now. To quickly go through research papers, a summary should be enough to cover all important points, but manual summaries of research papers are present in small amounts, to form the summaries researches would have to wait which will cause lag. So, we need a tool that can generate urgent summaries automatically which should be comprehensive, detailed, and accurate. that tool can also help researches retrieve relevant content

Automatic summary can deal with problems mentioned above. 
Choosing data for summary generation is a challenge. 
if whole research paper is used for summary generation, the system cost will be increased and might add redundant contents. 
if only abstracts are used, information loss would be increased compared with using full text.
so, in dataset can include citation content to know the insights of the current discussion, and to gain complete domain based knowledge of our topic. 
citation content is consistent to our current paper, provides more concepts such as entities and experimental methods, get original information.
some researchers applied citation content to generate summaries.

Tandon and Jain 2012 - generated structured summary by classifying citation contenct into one or more classes. 

Cohan and Goharian 2015 -  break citation content and their  context into groups , then ranked sentences within each group, then select sentences from each group to generate final summary.

Yang et al 2016 - used :
- key phrase (each word in its own, which groups together to form a phrase), 
- spectral clustering (a technique in graph theory, where the group of communities of nodes are formed in a graph, based on the edges connecting them) (google image : spectral clustering) 
- ILP optimization framework (integerr linear programming)
to generate summary

in this paper, citation content , and clustering is used for summary generation, called CitationAS,
work include : 
demonstration website which can generate automatic summary under a given topic. 
optimize a "search results clustering engine" in three areas : 1. merging of similar cluster label, 2. extraction of important sentences, 3. summary generation  (Osinki and Weiss 2005)

Dataset : 
collected 110,000 articles in xml format from PLOS One, dated 2006-2015
subjects covered are : biology, chemistry, mental health, Computer Science etc.
we first search for citation sentences, 4,339,217 citation sentences were extracted, which will be used as citation content for summary generation. 

*(look table 1 in Research Paper)

Framework of CitationAS:
user enters input.
then the tool search for relevant citation sentences for the input terms.
then the tool cluster (groups together) the same citation sentences which has similar topics.
then the tool merge the clusters which are similar with each other.
then the tool generate summary by selecting important sentences from each cluster.
evaluation is carried out by volunteers

* from figure 1 in Research Paper:

Retrieval Module : 
Lucene[6] to index and retrieve dataset.
during indexing files, also saved citation sentences, and their structure information (such as cited count, position of sentence in original article and paragraph)
for obtaining citation sentences associated with (or relevant to) input term, and scoring the citation sentences, we used built-in algorithms of Lucene. 
finally our software ranks results which will be used for next step of clustering.

clustering module :
first we apply Vector Space Model to represent citation sentences
and then used TF-IDF to calculate feature weights. 
in VSM, citation sentences are equivalent to documents and expressed as : 
Sj : jth citation sentence 
Ti : ith feature item (word)
Wij : weight of Ti in the jth Sentence or (Sj)
Sj  =  Sj  (  T1 , W1j  ;  T2 , W2j  ......;  Ti , Wij  ....;  Tm , Wmj  )

1<= j <= N ;  N is the no. of citation sentences
1<= i <= M ; M is the no. of feature items (words)

Wij = tfij * idfi ( log(N/ni + 0.01) )
tfij : frequency of ti in sentence sj
ni  : the number of sentences in which ti is located

bisecting K-means, Lingo, STC, built-in Carrot2      used to cluster (group together) citation sentences.
they groups together similar citation sentences in one group.

since VSM will represent documents in high dimensions (more axis). so we used NMF algorithm (Lee, 2000) to reduce dimensions.
NMF algo. obtains non-negative matrix after decomposing (breaking; into two or more matrix products)  the term-document matrix.
can be described as    A[m,n] =~ U[m,r] x V[r,n]
U is the base matrix, 
V is the coefficient matrix
r is the number of new feature item (word).
if r is less than m, A[m,n] is replaced with V[r,n] to reduce dimensions

Bisecting K-means (first look K-means from youtube ; 3-4 mins video ; get idea):
we use coefficient matrix (V) discussed above, to calculate similarity between citation sentence (Sj) and clustering centroid (centre of clusters)
Each sentence is assigned to most similar cluster. (grouping of similar sentences)
each cluster is assigned a label.
labels of each cluster are individual words, which are three feature items (words) with the greatest weight in Term-Document matrix, (Wij) (top three largest weights in each merged columns, citation sentences are represented in columns [j] whereas terms are represented in row [i] )

Lingo algorithm :
1. first extracts key phrases (important phrases) by suffix sorting array and longest common prefix array.
suffix sorting array is sorted array of suffixes if a word or phrase (suffix of dog$ => dog$, og$ , g$ , $ ,,,, and sorted is => $, dog$ , g$, og$ )
longest common prefix array (LCP) : [geeksforgeeks, geeks, geek, geezer] =>  LCP is : [gee] 

2. build term-phrase matrix based on key phrases, terms in in row and keyphrases are in column. weight of term i appearing in keyphrase j is in Matrix[i][j]. 
weight is calculated by TF-IDF.
3. then it constructs base vectors from term-phrase matrix. (vectors are vertical column of keyphrases represented as a linear array).
4. and it constructs base matrix through NMF (non negative matrix factorization).
Non negative matrix factorization can be represented as an example : 
	M [ words , documents ]   =   A [ words , topics ]    x   B [ topics , documents ]
5. finally each base vectors gets their words/ phrases to form one cluster label, 
    and sentence containing label's words. 
    then it will be assigned to their corresponding cluster of base vectors.

STC (Suffic tree clustering ) Algorithm : is based on generalized suffix tree -> recognises key words and phrases that occured more than once in citation sentences 
these words acts as one base clusters. 
there can be many same citation sentences in different clusters, whereas cluster labels are different.
so we merge these base clusters to form final clusters to reduce overlap of citation sentences.

Lingo, and STC both create overlapping clusters mean that one document can belong to two or more clusters while the cluster labels are different phrases.
Bisecting K-means create non overlaping clusters. and words in  cluster labels might not belong to all cluster's documents.

Cluster label generation :
two cluster labels might have similar semantic meaning such as "data mining approach"  and "data mining methods".
to improve accuracy, similar semantic level labels are merged.
to calculate semantic similarity between labels :
Word2Vec , WordNet

similarity computation based on Word2vec :
applies neural network to get two word vectors for given two labels.
two vectors contains words. then make them unit vectors -> to set them into same dimensionality.
then compute cosine similarity between those word vectors.
sim(p1,p2) = ( p1 . p2 )  / (   |p1| * |p2|   )

similarity computation based on wordNet :
wordnet is a semantic dictionary in which words are organised in a classification tree.
semantic similarity between words can be calculated by the path in the tree.

sim(w1,w2) = 1 / distance(w1,w2)

where distance is shortest path between words in the tree

then similarity between phrases is calculated using :
sim(p1,p2) = sim(p1i, p2j) /  (Lp1 * Lp2)
Lpi is the number of words in phrase pi 
pi is the phrase
p1i means ith word in phrase p1
linearly combine word2vec and wordnet to obtain a new similarity calculation method formula is given:
alpha is weight and we set it to 0.5
sim(p1,p2) = alpha * sim[word2vec] (p1,p2) + (1 - alpha)*(sim[wordnet] (p1,p2)  )


automatic summary generation :
clusters are sorted acc. to their size.
each cluster is taken as a paragraph and we merge them to generate final summary
we choose important citation sentences from each cluster for our final summary.
choosing of citation sentences from each cluster is based on methods to measure citation sentences scores

1. tf-idf based sentences extraction :
each citation sentence is represented by the term-document matrix. 
we compute sentence weight for the sentence = s = s (t1,w1 ; ti,wi, ... tm,wm)
and weight is computed by 
w(s) = sum ( i=1 to m ) (wi / m)
wi = ith word in a sentence S
m = no. of words in a sentence S
then citation sentences are ranked based on their weight.
the sentences with higher weight will be used as summary sentences.

2. mmr based sentences extraction:
maximum marginal relevance, *  look at page 5 of Reasearch paper
considers similarity between citation sentences with search items. 
and redundancy of sentences in summary
C is cluster of citattion sentences 
S is summary sentences 
C-S are the sentences not included in summary sentences
choose maximum of (similarity between citation sentences not chosen  for summary , and query)
with the maximum similarity between citation sentences not chosen  for summary , and citation sentences chosen for summary.

This method firstly selects maximum score of sentence as a summary sentence from the candidate sentence set, then it recalculates MMR value of the left sentences. When the candidate sentence set is empty, this algorithm ends.

In the cluster label generation test, we apply Davies-Bouldin (DB) and SC clustering index (Fahad et al., 2014) to find the best label generation method for each clustering algorithm.

lingo-tf-idf is the best performing method based on ranking

The reason may be that Lingo algorithm uses abstract matrix and the longest common prefix array when obtaining clustering labels, so that it can get more meaningful labels. In addition, citation sentence is assigned to the cluster containing corresponding labels, instead of calculating similarity between sentence and cluster centroid

Summary quality obtained by combination of STC, WordNet and TF-IDF or MMR is almost same, which indicates that sentences selection approaches do not have much impact on summary quality based on this clustering algorithm.

*read conclusion from Research Paper (page-8)