{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from os import walk\n",
    "import json\n",
    "import numpy as np\n",
    "from math import inf\n",
    "from math import sqrt,ceil\n",
    "from tkinter import *\n",
    "from tkinter import ttk\n",
    "from tkinter import filedialog\n",
    "import tkinter as tk\n",
    "import sys, os\n",
    "import string\n",
    "import collections\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter \n",
    "from numpy import ndarray\n",
    "global dirname\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "from spellchecker import SpellChecker\n",
    "import SemanticSimilarity as ss\n",
    "dirname=\"data\"\n",
    "r_flag=0\n",
    "\n",
    "def extract_citation_sentence():\n",
    "    try:\n",
    "        citation_sentence = []\n",
    "        numeric_flag = 0\n",
    "        #load files in chosen directory\n",
    "        files=[]\n",
    "        \n",
    "        if os.path.isdir(dirname) == False:\n",
    "            error_msg = \"The directory for indexing does not exist, please enter different path in the code line 22\"\n",
    "            print(error_msg)\n",
    "            l2.configure(text=error_msg)\n",
    "            \n",
    "        \n",
    "        for dirpath,dirnames,filenames in walk(dirname):  \n",
    "            for j in filenames:\n",
    "                files.append(j)\n",
    "\n",
    "        for file in files:\n",
    "\n",
    "            f = open(dirname+'/'+file,'r')\n",
    "\n",
    "            x = f.read()\n",
    "            for sentence in x.split(\".\"):\n",
    "                for i in range(len(sentence)):\n",
    "                    if sentence[i] == '[':\n",
    "                        for j in range(1,5):\n",
    "                            if i + j >= len(sentence):\n",
    "                                break\n",
    "                            if sentence[i+j].isnumeric():\n",
    "                                numeric_flag = 1\n",
    "                            elif sentence[i+j] == ']' and numeric_flag == 1:\n",
    "                                numeric_flag = 0\n",
    "                                citation_sentence.append(sentence[:i]+sentence[i+j+1:])\n",
    "                                break\n",
    "                            else:\n",
    "                                break\n",
    "                        if(numeric_flag == 0):\n",
    "                            break\n",
    "            f.close()\n",
    "\n",
    "        return citation_sentence\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"in extract_citation_sentence\")\n",
    "        print(e)\n",
    "        exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "        print(exc_type, fname, exc_tb.tb_lineno)\n",
    "\n",
    "def read_cache():\n",
    "    try:\n",
    "        #loading saved compressed dictionary of numpy arrays\n",
    "        loaded = np.load(dirname+'.npz',allow_pickle=True)\n",
    "        return 1,loaded\n",
    "    except Exception as e:\n",
    "        print(\"in read cache\")\n",
    "        print(e)\n",
    "        exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "        print(exc_type, fname, exc_tb.tb_lineno)\n",
    "        return 0,None\n",
    "def write_cache(to_save):\n",
    "    try:\n",
    "        #saving compressed dictionary of numpy arrays \n",
    "        np.savez_compressed(dirname,terms=np.array(to_save['terms']),weight=to_save['weight'],tf=to_save['tf'],idf=to_save['idf'],df=to_save['df'],doc_norm=to_save['doc_norm'],doc_length=to_save['doc_length'],citation_sentences=to_save['citation_sentences'])\n",
    "    except Exception as e:\n",
    "        print(\"in write cache\")\n",
    "        print(e)\n",
    "        exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "        print(exc_type, fname, exc_tb.tb_lineno)\n",
    "\n",
    "def simplify(sentence):\n",
    "    #remove irrelevent symbols from sentence\n",
    "    exclude = set(string.punctuation)\n",
    "    sentence = ''.join(ch.lower() for ch in sentence if ch not in exclude and not ch.isnumeric())\n",
    "    return sentence\n",
    "\n",
    "def cosine_similarity(x,y):\n",
    "    #sim (x,y) = x.y / |x|*|y|\n",
    "    x=np.array(x)\n",
    "    y=np.array(y)\n",
    "    x_norm=np.linalg.norm(x)\n",
    "    y_norm=np.linalg.norm(y)\n",
    "    result=(x*y)/(x_norm*y_norm)\n",
    "    return np.sum(result)\n",
    "\n",
    "def preprocessing(word):\n",
    "    try:\n",
    "        lemma = WordNetLemmatizer()\n",
    "        return lemma.lemmatize((lemma.lemmatize(word.lower(), pos = 'n')), pos = 'v')\n",
    "    except Exception as e:\n",
    "        print(\"in preprocessing\")\n",
    "        print(e)\n",
    "        exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "        print(exc_type, fname, exc_tb.tb_lineno)\n",
    "        return None\n",
    "\n",
    "def reindex():\n",
    "    try:\n",
    "        \n",
    "        if os.path.isdir(dirname) == False:\n",
    "            error_msg = \"The directory for indexing does not exist, please enter different path in the code line 22\"\n",
    "            print(error_msg)\n",
    "            l2.configure(text=error_msg)\n",
    "            return\n",
    "            \n",
    "        os.remove(dirname+'.npz')\n",
    "        indexing()\n",
    "    except Exception as e:\n",
    "        print(\"in preprocessing\")\n",
    "        print(e)\n",
    "        exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "        print(exc_type, fname, exc_tb.tb_lineno)\n",
    "        indexing()\n",
    "\n",
    "def indexing():\n",
    "    global dirname\n",
    "    global r_flag\n",
    "    try:\n",
    "        print(\"Processing...\")\n",
    "        doc_number =-1\n",
    "        N=0\n",
    "        to_save={}\n",
    "        \n",
    "        #load saved dictionary\n",
    "        read_flag, to_save = read_cache()\n",
    "        \n",
    "        if(read_flag==1):\n",
    "            return to_save, len(to_save['doc_norm'])\n",
    "        \n",
    "        citation_sentences = extract_citation_sentence()\n",
    "        N = len(citation_sentences)\n",
    "        simplified_citation_sentences = []\n",
    "        \n",
    "        #save every term from every document/ citation_sentence\n",
    "        terms2d=[[] for i in range(N)]\n",
    "        for sentences in citation_sentences:           \n",
    "            doc_number=doc_number+1\n",
    "            sentence=simplify(sentences)\n",
    "            simplified_citation_sentences.append(sentence)\n",
    "            for word in sentence.split():\n",
    "                #preprocess the word\n",
    "                new_word=preprocessing(word)\n",
    "                terms2d[doc_number].append(new_word)\n",
    "        \n",
    "        #pad the terms array to perfect square size to vectorize the operations\n",
    "        pad = len(max(terms2d, key=len))\n",
    "        terms=np.array([i + [\"-1marker\"]*(pad-len(i)) for i in terms2d])\n",
    "        terms=np.reshape(terms,-1)\n",
    "        terms=terms[terms != \"-1marker\"]\n",
    "        terms=np.array(list(set(terms)))\n",
    "        \n",
    "        #initialize term frequency for every document and term\n",
    "        tf=[[0 for i in range(len(terms))] for i in range(N)]\n",
    "        for doc_no in range(len(terms2d)):\n",
    "            for word in terms2d[doc_no]:\n",
    "                term_index=list(terms).index(word)\n",
    "                tf[doc_no][term_index]=tf[doc_no][term_index]+1\n",
    "                \n",
    "        tf=np.array(tf)\n",
    "        #calculate length of document\n",
    "        doc_length=np.sum(tf,axis=1)\n",
    "        #calculate the length of documents term t appears in\n",
    "        df=np.sum(tf.astype(bool),axis=0)\n",
    "        #calculate idf\n",
    "        idf=np.log10(N/df)\n",
    "        #calculate weight by idf* normalized tf\n",
    "        weight=(tf/np.reshape(doc_length,(len(doc_length),1)))*idf\n",
    "        #Transpose of weight\n",
    "        weight=np.reshape(weight,(N,-1))\n",
    "        #calculate doc_norm\n",
    "        doc_norm=np.linalg.norm(weight,axis=1)\n",
    "        \n",
    "        #save the dictionary of numpy arrays\n",
    "        to_save={}\n",
    "        if(read_flag==0):\n",
    "            to_save['terms']=terms\n",
    "            to_save['tf']=tf\n",
    "            to_save['df']=df\n",
    "            to_save['idf']=idf\n",
    "            to_save['doc_length']=doc_length\n",
    "            to_save['doc_norm']=doc_norm\n",
    "            to_save['weight']=weight\n",
    "            to_save['citation_sentences']=simplified_citation_sentences\n",
    "            write_cache(to_save)\n",
    "        return to_save,len(doc_norm)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"in indexing\")\n",
    "        print(e)\n",
    "        exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "        print(exc_type, fname, exc_tb.tb_lineno)\n",
    "\n",
    "def process_query(query):\n",
    "    try:\n",
    "        global labelFrame\n",
    "        #index the files in chosen directory\n",
    "        saved,N=indexing()\n",
    "        #load the saved numpy arrays\n",
    "        terms=saved['terms']\n",
    "        idf=saved['idf']\n",
    "        weight=saved['weight']\n",
    "        tf=saved['tf']\n",
    "        df=saved['df']\n",
    "        doc_length=saved['doc_length']\n",
    "        citation_sentences=saved['citation_sentences']\n",
    "        \n",
    "        #return if no query is entered\n",
    "        if(len(query)==0):\n",
    "            return\n",
    "        #create vector of term frequency of  query words\n",
    "        tfq=np.zeros(len(terms))\n",
    "        weight_q=np.zeros(len(terms))\n",
    "        query_words=[]\n",
    "        query_simplified=simplify(query)\n",
    "        for words in query_simplified.split():\n",
    "            new_word=preprocessing(words)\n",
    "            query_words.append(new_word)\n",
    "            if new_word in terms:\n",
    "                index=list(terms).index(new_word)\n",
    "                tfq[index]=tfq[index]+1\n",
    "                \n",
    "        #calculate weight of query\n",
    "        weight_q = (tfq / np.sum(tfq) ) * idf\n",
    "        weight_q = np.reshape(weight_q,(1,-1))\n",
    "        scores = np.zeros(N)\n",
    "        t_count = 0\n",
    "        \n",
    "        for i in range(N):\n",
    "            temp = np.reshape( weight[i], (1, -1))\n",
    "            #call similarity function to get score\n",
    "            if cosine_similarity(weight_q, temp) > 0:\n",
    "                scores[i] = cosine_similarity(weight_q, temp)\n",
    "        \n",
    "        score_dict = {}\n",
    "        for i in range(N):\n",
    "            if scores[i] > 0:\n",
    "                score_dict[i] = scores[i]\n",
    "        \n",
    "        sorted_citation_sentences = []\n",
    "        sorted_citation_sentences_weight = []\n",
    "        sorted_citation_sentences_index = []\n",
    "        sorted_citation_sentences_score = []\n",
    "        \n",
    "        #sort the scores and get the corresponding documents\n",
    "        for j in sorted(score_dict, key = score_dict.get, reverse=True):\n",
    "            sorted_citation_sentences.append(citation_sentences[j])\n",
    "            sorted_citation_sentences_weight.append(weight[j])\n",
    "            sorted_citation_sentences_index.append(j)\n",
    "            sorted_citation_sentences_score.append(score_dict[j])\n",
    "        \n",
    "        return sorted_citation_sentences, sorted_citation_sentences_weight, sorted_citation_sentences_index, sorted_citation_sentences_score, terms\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"in process query\")\n",
    "        print(e)\n",
    "        exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "        print(exc_type, fname, exc_tb.tb_lineno)\n",
    "\n",
    "def optimal_clusters(w):\n",
    "    sil = []\n",
    "    kmax = len(w)-1\n",
    "\n",
    "    # dissimilarity would not be defined for a single cluster, thus, minimum number of clusters should be 2\n",
    "    for k in range(2, kmax+1):\n",
    "        kmeans = KMeans(n_clusters = k).fit(w)\n",
    "        labels = kmeans.labels_\n",
    "        sil.append(silhouette_score(w, labels, metric = 'cosine'))\n",
    "        \n",
    "    return sil.index(max(sil))+2\n",
    "\n",
    "def clustering():\n",
    "    try:\n",
    "        query = str(e1.get())\n",
    "        if(len(query) < 2 or query == None):\n",
    "            return\n",
    "        cs, w, i ,s, t = process_query(query)\n",
    "        \n",
    "        if(len(i)<=1): \n",
    "            error_msg = \"No related sentence is found, please enter another query term/s, if problem stays, please re-index\"\n",
    "            print(error_msg)\n",
    "            l2.configure(text=error_msg)\n",
    "            raise \"none\"\n",
    "            \n",
    "        num_clusters = optimal_clusters(w)\n",
    "        km = KMeans(n_clusters=num_clusters)\n",
    "        km.fit(w)\n",
    "        clusters = km.labels_.tolist()\n",
    "\n",
    "        cluster_keywords = []\n",
    "\n",
    "        sum_ = [[0 for i in range(len(t))] for x in range(num_clusters)]\n",
    "        for x in clusters:\n",
    "            w_ = list(l for l in range(len(clusters)) if clusters[l] == x )\n",
    "            for y in w_:\n",
    "                sum_[x] += w[y] \n",
    "            cluster_keywords.append(t[list(sum_[x]).index(max(sum_[x]))])\n",
    "            \n",
    "        return clusters, cluster_keywords, cs ,w, i, s, t\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"in clustering\")\n",
    "        print(e)\n",
    "        exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "        print(exc_type, fname, exc_tb.tb_lineno)\n",
    "\n",
    "def merge_cluster(clusters, cluster_keywords ):\n",
    "    try:\n",
    "        for x in range(len(clusters)):\n",
    "            for y in range(len(clusters)):\n",
    "                if x == y or clusters[x] == clusters[y]:\n",
    "                    continue\n",
    "                sim = ss.semanticSimilarity(cluster_keywords[x],cluster_keywords[y])\n",
    "                if(sim > 0.25):\n",
    "                    clusters[y] = clusters[x]\n",
    "                    cluster_keywords[y] = cluster_keywords[x]\n",
    "        return clusters, cluster_keywords\n",
    "    except Exception as e:\n",
    "        print(\"in merge cluster\")\n",
    "        print(e)\n",
    "        exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "        print(exc_type, fname, exc_tb.tb_lineno)\n",
    "\n",
    "def cluster_minimized( clusters):\n",
    "    try:\n",
    "        cluster_sorted = sorted(clusters)\n",
    "        cluster_new    = [0 for x in range(len(cluster_sorted))]\n",
    "        cluster_replaced    = [0 for x in range(len(cluster_sorted))]\n",
    "\n",
    "        for x in range(1,len(cluster_new)):\n",
    "            if cluster_sorted[x] > cluster_sorted[x-1]:\n",
    "                cluster_new[x] = cluster_new[x-1]+1\n",
    "            else:\n",
    "                cluster_new[x] = cluster_new[x-1]\n",
    "\n",
    "        for x in range(len(clusters)):\n",
    "            cluster_replaced[x] = cluster_new[cluster_sorted.index( clusters[x] ) ]\n",
    "        clusters=cluster_replaced\n",
    "\n",
    "        return clusters\n",
    "    except Exception as e:\n",
    "        print(\"in cluster minimized\")\n",
    "        print(e)\n",
    "        exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "        print(exc_type, fname, exc_tb.tb_lineno)\n",
    "\n",
    "def generate_summary(event=\"\"):\n",
    "    try:\n",
    "        clusters, cluster_keywords, cs ,w, i, s, t = clustering()\n",
    "\n",
    "        clusters, cluster_keywords = merge_cluster( clusters, cluster_keywords)\n",
    "        clusters = cluster_minimized( clusters )\n",
    "\n",
    "        merged_vectors = [[] for x in range(len(set(clusters)))]\n",
    "        merged_vectors_index = [[] for x in range(len(set(clusters)))]\n",
    "\n",
    "        for x in range(len(clusters)):\n",
    "            merged_vectors[clusters[x]].append(w[x])\n",
    "            merged_vectors_index[clusters[x]].append(x)\n",
    "\n",
    "        summary = []\n",
    "        \n",
    "        for x in range(len(merged_vectors)):\n",
    "            sum_ = 0\n",
    "            score_dict = {}\n",
    "            for y in range(len(merged_vectors[x])):\n",
    "                score_dict[merged_vectors_index[x][y]] =  np.sum(merged_vectors[x][y])\n",
    "\n",
    "            for j in sorted(score_dict, key=score_dict.get, reverse=True)[:3]:\n",
    "                summary.append(cs[j].replace('\\n',' '))\n",
    "\n",
    "        spell = SpellChecker()\n",
    "        new_summary=[]\n",
    "\n",
    "        summary = '. '.join(summary)\n",
    "        for words in summary.split():\n",
    "            new_summary.append(spell.correction(words))\n",
    "\n",
    "        print(\" \".join(new_summary))\n",
    "        l2.configure(text=new_summary)\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = \"No related sentence is found, please enter another query term/s, if problem stays, please re-index\"\n",
    "        print(error_msg)\n",
    "        l2.configure(text=error_msg)\n",
    "        print(\"in generate summary\")\n",
    "        print(e)\n",
    "        exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "        print(exc_type, fname, exc_tb.tb_lineno)\n",
    "\n",
    "\n",
    "try:\n",
    "    master = Tk()\n",
    "    master.title(\"CitationAS Demo\")\n",
    "    master.minsize(500, 400)\n",
    "\n",
    "    labelFrame=ttk.LabelFrame(master,text=\"\")\n",
    "    labelFrame.pack(fill=X)\n",
    "\n",
    "    b0 = Button(labelFrame, text = \"Re-Index\",command=reindex, bg =\"black\",fg=\"light blue\",height=1)\n",
    "    b0.pack(side=LEFT)\n",
    "\n",
    "    l = Label(labelFrame,text=\"Summary Generator\", bg = \"green\" , fg=\"white\",height=1)\n",
    "    l.pack(fill=BOTH, expand=True)\n",
    "\n",
    "    labelFrame2=ttk.LabelFrame(master,text=\"\")\n",
    "    labelFrame2.pack(fill=X)\n",
    "\n",
    "    master.bind('<Return>', None)\n",
    "\n",
    "    b1 = Button(labelFrame2, text = \"Run Query\",command=generate_summary, bg =\"black\",fg=\"light blue\")\n",
    "    b1.grid(row=4,column=5)\n",
    "\n",
    "    master.bind('<Return>', generate_summary)\n",
    "\n",
    "    Label(labelFrame2,text=\"Input\").grid(row = 4,column = 0, padx=10,pady=20, sticky=\"w\")\n",
    "\n",
    "    e1 = Entry(labelFrame2)\n",
    "    e1.grid(row = 4,column = 3, padx=10,pady=20 ,sticky=\"nsew\")\n",
    "    e1.focus()\n",
    "\n",
    "    labelFrame2.grid_rowconfigure(4, weight=10)\n",
    "    labelFrame2.grid_columnconfigure(3, weight=10)\n",
    "\n",
    "    size_x = 500\n",
    "\n",
    "    labelFrame3=ttk.LabelFrame(master,text=\"\")\n",
    "    labelFrame3.pack(fill=X)\n",
    "\n",
    "    Label(labelFrame3,text=\"Output\").grid(row = 4,column = 0, padx=5,pady=20, sticky=\"w\")\n",
    "\n",
    "    l2 = Label(labelFrame3,text=\"\",wraplength=size_x)\n",
    "    l2.grid(row = 4,column = 3, padx=10,pady=20 ,sticky=\"w\")\n",
    "\n",
    "\n",
    "    labelFrame3.grid_rowconfigure(4, weight=1)\n",
    "    labelFrame3.grid_columnconfigure(3, weight=1)\n",
    "\n",
    "    mainloop()\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"in Main\")\n",
    "    print(e)\n",
    "    exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "    fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "    print(exc_type, fname, exc_tb.tb_lineno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
